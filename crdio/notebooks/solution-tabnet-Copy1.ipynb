{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv', 'test.csv', 'train.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    " #TabNetRegressor()\n",
    "#clf.fit(\n",
    "#  X_train, Y_train,\n",
    "#  eval_set=[(X_valid, y_valid)]\n",
    "#)\n",
    "#preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcolumns = list(test.columns)\n",
    "ycolumns = \"NSP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Gini(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"gini\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        #print(y_true, np.argmax(y_score, 1))\n",
    "        n_size = np.max(y_true)+1\n",
    "        y_true_ = np.eye(n_size)[y_true]\n",
    "        auc = roc_auc_score(y_true_, y_score, average='macro', multi_class=\"ovr\")\n",
    "        return max(2*auc - 1, 0.)\n",
    "\n",
    "class F1(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"f1\"\n",
    "        self._maximize = True\n",
    "        \n",
    "    def __call__(self, y_true, y_score):\n",
    "        return f1_score(y_true, np.argmax(y_score, 1), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train[xcolumns].values\n",
    "y = train[ycolumns].values\n",
    "X_test = test[xcolumns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBE</th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>MLTV</th>\n",
       "      <th>DL</th>\n",
       "      <th>...</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>133.219412</td>\n",
       "      <td>133.219412</td>\n",
       "      <td>2.684118</td>\n",
       "      <td>6.851176</td>\n",
       "      <td>3.597059</td>\n",
       "      <td>46.857647</td>\n",
       "      <td>1.334118</td>\n",
       "      <td>9.872353</td>\n",
       "      <td>8.207353</td>\n",
       "      <td>1.527647</td>\n",
       "      <td>...</td>\n",
       "      <td>93.644118</td>\n",
       "      <td>163.952941</td>\n",
       "      <td>4.105294</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>137.171176</td>\n",
       "      <td>134.485882</td>\n",
       "      <td>137.935882</td>\n",
       "      <td>18.837647</td>\n",
       "      <td>0.313529</td>\n",
       "      <td>1.302353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.845934</td>\n",
       "      <td>9.845934</td>\n",
       "      <td>3.452645</td>\n",
       "      <td>34.902431</td>\n",
       "      <td>2.788713</td>\n",
       "      <td>17.164055</td>\n",
       "      <td>0.878399</td>\n",
       "      <td>18.577602</td>\n",
       "      <td>5.698527</td>\n",
       "      <td>2.430154</td>\n",
       "      <td>...</td>\n",
       "      <td>29.728492</td>\n",
       "      <td>17.910908</td>\n",
       "      <td>2.980430</td>\n",
       "      <td>0.706449</td>\n",
       "      <td>16.542794</td>\n",
       "      <td>15.694191</td>\n",
       "      <td>14.535621</td>\n",
       "      <td>29.648333</td>\n",
       "      <td>0.609851</td>\n",
       "      <td>0.615950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>133.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>93.500000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>50.700000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               LBE           LB           AC           FM           UC  \\\n",
       "count  1700.000000  1700.000000  1700.000000  1700.000000  1700.000000   \n",
       "mean    133.219412   133.219412     2.684118     6.851176     3.597059   \n",
       "std       9.845934     9.845934     3.452645    34.902431     2.788713   \n",
       "min     106.000000   106.000000     0.000000     0.000000     0.000000   \n",
       "25%     126.000000   126.000000     0.000000     0.000000     1.000000   \n",
       "50%     133.000000   133.000000     1.000000     0.000000     3.000000   \n",
       "75%     140.000000   140.000000     4.000000     2.000000     5.000000   \n",
       "max     160.000000   160.000000    19.000000   564.000000    23.000000   \n",
       "\n",
       "              ASTV         MSTV         ALTV         MLTV           DL  ...  \\\n",
       "count  1700.000000  1700.000000  1700.000000  1700.000000  1700.000000  ...   \n",
       "mean     46.857647     1.334118     9.872353     8.207353     1.527647  ...   \n",
       "std      17.164055     0.878399    18.577602     5.698527     2.430154  ...   \n",
       "min      12.000000     0.200000     0.000000     0.000000     0.000000  ...   \n",
       "25%      32.000000     0.700000     0.000000     4.500000     0.000000  ...   \n",
       "50%      48.000000     1.200000     0.000000     7.400000     0.000000  ...   \n",
       "75%      61.000000     1.700000    10.250000    10.800000     2.250000  ...   \n",
       "max      86.000000     7.000000    91.000000    50.700000    14.000000  ...   \n",
       "\n",
       "               Min          Max         Nmax       Nzeros         Mode  \\\n",
       "count  1700.000000  1700.000000  1700.000000  1700.000000  1700.000000   \n",
       "mean     93.644118   163.952941     4.105294     0.320000   137.171176   \n",
       "std      29.728492    17.910908     2.980430     0.706449    16.542794   \n",
       "min      50.000000   123.000000     0.000000     0.000000    60.000000   \n",
       "25%      67.000000   152.000000     2.000000     0.000000   129.000000   \n",
       "50%      93.500000   162.000000     3.000000     0.000000   139.000000   \n",
       "75%     120.000000   174.000000     6.000000     0.000000   148.000000   \n",
       "max     158.000000   238.000000    18.000000    10.000000   187.000000   \n",
       "\n",
       "              Mean       Median     Variance     Tendency          NSP  \n",
       "count  1700.000000  1700.000000  1700.000000  1700.000000  1700.000000  \n",
       "mean    134.485882   137.935882    18.837647     0.313529     1.302353  \n",
       "std      15.694191    14.535621    29.648333     0.609851     0.615950  \n",
       "min      73.000000    77.000000     0.000000    -1.000000     1.000000  \n",
       "25%     125.000000   128.000000     2.000000     0.000000     1.000000  \n",
       "50%     136.000000   139.000000     7.000000     0.000000     1.000000  \n",
       "75%     145.000000   148.000000    24.000000     1.000000     1.000000  \n",
       "max     182.000000   186.000000   269.000000     1.000000     3.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "transformer=QuantileTransformer(n_quantiles=400, output_distribution='normal')\n",
    "X_t = transformer.fit_transform(X)\n",
    "X_test_t = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 1.31976 | val_0_f1: 0.33406 | val_0_gini: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 1.03838 | val_0_f1: 0.30846 | val_0_gini: 0.13354 |  0:00:00s\n",
      "epoch 2  | loss: 0.90338 | val_0_f1: 0.38981 | val_0_gini: 0.2627  |  0:00:00s\n",
      "epoch 3  | loss: 0.86537 | val_0_f1: 0.53657 | val_0_gini: 0.33117 |  0:00:00s\n",
      "epoch 4  | loss: 0.82035 | val_0_f1: 0.58602 | val_0_gini: 0.24709 |  0:00:01s\n",
      "epoch 5  | loss: 0.73239 | val_0_f1: 0.66691 | val_0_gini: 0.46408 |  0:00:01s\n",
      "epoch 6  | loss: 0.68299 | val_0_f1: 0.6621  | val_0_gini: 0.54042 |  0:00:01s\n",
      "epoch 7  | loss: 0.68114 | val_0_f1: 0.672   | val_0_gini: 0.61836 |  0:00:01s\n",
      "epoch 8  | loss: 0.62297 | val_0_f1: 0.64301 | val_0_gini: 0.63453 |  0:00:02s\n",
      "epoch 9  | loss: 0.59369 | val_0_f1: 0.65554 | val_0_gini: 0.69495 |  0:00:02s\n",
      "epoch 10 | loss: 0.59073 | val_0_f1: 0.64496 | val_0_gini: 0.72158 |  0:00:02s\n",
      "epoch 11 | loss: 0.5665  | val_0_f1: 0.67115 | val_0_gini: 0.77164 |  0:00:03s\n",
      "epoch 12 | loss: 0.5153  | val_0_f1: 0.74311 | val_0_gini: 0.79435 |  0:00:03s\n",
      "epoch 13 | loss: 0.49162 | val_0_f1: 0.76054 | val_0_gini: 0.79236 |  0:00:03s\n",
      "epoch 14 | loss: 0.4899  | val_0_f1: 0.77576 | val_0_gini: 0.77341 |  0:00:03s\n",
      "epoch 15 | loss: 0.47129 | val_0_f1: 0.75718 | val_0_gini: 0.81322 |  0:00:04s\n",
      "epoch 16 | loss: 0.43337 | val_0_f1: 0.76175 | val_0_gini: 0.82863 |  0:00:04s\n",
      "epoch 17 | loss: 0.42673 | val_0_f1: 0.77245 | val_0_gini: 0.86766 |  0:00:04s\n",
      "epoch 18 | loss: 0.42528 | val_0_f1: 0.77527 | val_0_gini: 0.87962 |  0:00:04s\n",
      "epoch 19 | loss: 0.43559 | val_0_f1: 0.79991 | val_0_gini: 0.89208 |  0:00:05s\n",
      "epoch 20 | loss: 0.40015 | val_0_f1: 0.80025 | val_0_gini: 0.89043 |  0:00:05s\n",
      "epoch 21 | loss: 0.40741 | val_0_f1: 0.80127 | val_0_gini: 0.87933 |  0:00:05s\n",
      "epoch 22 | loss: 0.3874  | val_0_f1: 0.79399 | val_0_gini: 0.85622 |  0:00:05s\n",
      "epoch 23 | loss: 0.32738 | val_0_f1: 0.7847  | val_0_gini: 0.84992 |  0:00:06s\n",
      "epoch 24 | loss: 0.36524 | val_0_f1: 0.7984  | val_0_gini: 0.86264 |  0:00:06s\n",
      "epoch 25 | loss: 0.35733 | val_0_f1: 0.79525 | val_0_gini: 0.87855 |  0:00:06s\n",
      "epoch 26 | loss: 0.35384 | val_0_f1: 0.80218 | val_0_gini: 0.87514 |  0:00:06s\n",
      "epoch 27 | loss: 0.31594 | val_0_f1: 0.8194  | val_0_gini: 0.87334 |  0:00:07s\n",
      "epoch 28 | loss: 0.32269 | val_0_f1: 0.81875 | val_0_gini: 0.87304 |  0:00:07s\n",
      "epoch 29 | loss: 0.34691 | val_0_f1: 0.81203 | val_0_gini: 0.8725  |  0:00:07s\n",
      "\n",
      "Early stopping occured at epoch 29 with best_epoch = 19 and best_val_0_gini = 0.89208\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.3047  | val_0_f1: 0.44832 | val_0_gini: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 1.05704 | val_0_f1: 0.43652 | val_0_gini: 0.0     |  0:00:00s\n",
      "epoch 2  | loss: 0.94324 | val_0_f1: 0.50542 | val_0_gini: 0.00652 |  0:00:00s\n",
      "epoch 3  | loss: 0.83922 | val_0_f1: 0.58871 | val_0_gini: 0.17866 |  0:00:00s\n",
      "epoch 4  | loss: 0.75206 | val_0_f1: 0.6537  | val_0_gini: 0.30049 |  0:00:01s\n",
      "epoch 5  | loss: 0.6865  | val_0_f1: 0.63472 | val_0_gini: 0.33634 |  0:00:01s\n",
      "epoch 6  | loss: 0.63526 | val_0_f1: 0.66037 | val_0_gini: 0.35527 |  0:00:01s\n",
      "epoch 7  | loss: 0.61234 | val_0_f1: 0.63613 | val_0_gini: 0.3674  |  0:00:01s\n",
      "epoch 8  | loss: 0.54261 | val_0_f1: 0.58189 | val_0_gini: 0.25735 |  0:00:02s\n",
      "epoch 9  | loss: 0.53647 | val_0_f1: 0.53336 | val_0_gini: 0.25567 |  0:00:02s\n",
      "epoch 10 | loss: 0.52573 | val_0_f1: 0.53537 | val_0_gini: 0.32628 |  0:00:02s\n",
      "epoch 11 | loss: 0.48355 | val_0_f1: 0.59928 | val_0_gini: 0.42977 |  0:00:02s\n",
      "epoch 12 | loss: 0.46091 | val_0_f1: 0.60445 | val_0_gini: 0.45359 |  0:00:03s\n",
      "epoch 13 | loss: 0.43332 | val_0_f1: 0.60457 | val_0_gini: 0.42521 |  0:00:03s\n",
      "epoch 14 | loss: 0.40762 | val_0_f1: 0.62764 | val_0_gini: 0.46464 |  0:00:03s\n",
      "epoch 15 | loss: 0.43756 | val_0_f1: 0.6357  | val_0_gini: 0.42854 |  0:00:03s\n",
      "epoch 16 | loss: 0.45873 | val_0_f1: 0.64924 | val_0_gini: 0.40421 |  0:00:04s\n",
      "epoch 17 | loss: 0.39832 | val_0_f1: 0.66299 | val_0_gini: 0.49187 |  0:00:04s\n",
      "epoch 18 | loss: 0.3731  | val_0_f1: 0.6685  | val_0_gini: 0.5335  |  0:00:04s\n",
      "epoch 19 | loss: 0.37845 | val_0_f1: 0.70536 | val_0_gini: 0.6536  |  0:00:04s\n",
      "epoch 20 | loss: 0.4191  | val_0_f1: 0.67217 | val_0_gini: 0.6005  |  0:00:05s\n",
      "epoch 21 | loss: 0.4009  | val_0_f1: 0.68147 | val_0_gini: 0.56214 |  0:00:05s\n",
      "epoch 22 | loss: 0.35027 | val_0_f1: 0.70167 | val_0_gini: 0.59703 |  0:00:05s\n",
      "epoch 23 | loss: 0.35178 | val_0_f1: 0.71897 | val_0_gini: 0.62106 |  0:00:05s\n",
      "epoch 24 | loss: 0.38067 | val_0_f1: 0.76043 | val_0_gini: 0.59565 |  0:00:06s\n",
      "epoch 25 | loss: 0.37311 | val_0_f1: 0.77193 | val_0_gini: 0.5033  |  0:00:06s\n",
      "epoch 26 | loss: 0.34166 | val_0_f1: 0.7584  | val_0_gini: 0.47648 |  0:00:06s\n",
      "epoch 27 | loss: 0.35026 | val_0_f1: 0.78143 | val_0_gini: 0.55914 |  0:00:06s\n",
      "epoch 28 | loss: 0.35561 | val_0_f1: 0.767   | val_0_gini: 0.46681 |  0:00:07s\n",
      "epoch 29 | loss: 0.34404 | val_0_f1: 0.75525 | val_0_gini: 0.44304 |  0:00:07s\n",
      "\n",
      "Early stopping occured at epoch 29 with best_epoch = 19 and best_val_0_gini = 0.6536\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.33553 | val_0_f1: 0.47888 | val_0_gini: 0.06723 |  0:00:00s\n",
      "epoch 1  | loss: 1.06315 | val_0_f1: 0.42134 | val_0_gini: 0.08341 |  0:00:00s\n",
      "epoch 2  | loss: 0.93099 | val_0_f1: 0.38036 | val_0_gini: 0.06297 |  0:00:00s\n",
      "epoch 3  | loss: 0.84207 | val_0_f1: 0.44626 | val_0_gini: 0.15772 |  0:00:00s\n",
      "epoch 4  | loss: 0.76863 | val_0_f1: 0.59003 | val_0_gini: 0.25178 |  0:00:01s\n",
      "epoch 5  | loss: 0.72612 | val_0_f1: 0.64517 | val_0_gini: 0.27048 |  0:00:01s\n",
      "epoch 6  | loss: 0.69716 | val_0_f1: 0.66814 | val_0_gini: 0.29764 |  0:00:01s\n",
      "epoch 7  | loss: 0.64309 | val_0_f1: 0.65563 | val_0_gini: 0.27296 |  0:00:01s\n",
      "epoch 8  | loss: 0.59505 | val_0_f1: 0.61657 | val_0_gini: 0.29175 |  0:00:02s\n",
      "epoch 9  | loss: 0.55357 | val_0_f1: 0.67467 | val_0_gini: 0.42018 |  0:00:02s\n",
      "epoch 10 | loss: 0.51864 | val_0_f1: 0.71785 | val_0_gini: 0.49776 |  0:00:02s\n",
      "epoch 11 | loss: 0.47555 | val_0_f1: 0.74627 | val_0_gini: 0.54865 |  0:00:02s\n",
      "epoch 12 | loss: 0.47798 | val_0_f1: 0.75429 | val_0_gini: 0.58522 |  0:00:03s\n",
      "epoch 13 | loss: 0.4716  | val_0_f1: 0.7537  | val_0_gini: 0.57104 |  0:00:03s\n",
      "epoch 14 | loss: 0.44781 | val_0_f1: 0.7414  | val_0_gini: 0.47396 |  0:00:03s\n",
      "epoch 15 | loss: 0.43522 | val_0_f1: 0.75251 | val_0_gini: 0.44257 |  0:00:04s\n",
      "epoch 16 | loss: 0.42748 | val_0_f1: 0.74048 | val_0_gini: 0.43932 |  0:00:04s\n",
      "epoch 17 | loss: 0.44163 | val_0_f1: 0.73491 | val_0_gini: 0.55925 |  0:00:04s\n",
      "epoch 18 | loss: 0.3523  | val_0_f1: 0.76852 | val_0_gini: 0.63134 |  0:00:05s\n",
      "epoch 19 | loss: 0.40759 | val_0_f1: 0.74684 | val_0_gini: 0.68906 |  0:00:05s\n",
      "epoch 20 | loss: 0.35999 | val_0_f1: 0.7405  | val_0_gini: 0.698   |  0:00:05s\n",
      "epoch 21 | loss: 0.39592 | val_0_f1: 0.72097 | val_0_gini: 0.71269 |  0:00:05s\n",
      "epoch 22 | loss: 0.3577  | val_0_f1: 0.7423  | val_0_gini: 0.73143 |  0:00:06s\n",
      "epoch 23 | loss: 0.33658 | val_0_f1: 0.77086 | val_0_gini: 0.7774  |  0:00:06s\n",
      "epoch 24 | loss: 0.3116  | val_0_f1: 0.78071 | val_0_gini: 0.78446 |  0:00:06s\n",
      "epoch 25 | loss: 0.35326 | val_0_f1: 0.78598 | val_0_gini: 0.72993 |  0:00:06s\n",
      "epoch 26 | loss: 0.32502 | val_0_f1: 0.77702 | val_0_gini: 0.75534 |  0:00:07s\n",
      "epoch 27 | loss: 0.35786 | val_0_f1: 0.78586 | val_0_gini: 0.81641 |  0:00:07s\n",
      "epoch 28 | loss: 0.31172 | val_0_f1: 0.82949 | val_0_gini: 0.844   |  0:00:07s\n",
      "epoch 29 | loss: 0.34704 | val_0_f1: 0.84407 | val_0_gini: 0.84542 |  0:00:08s\n",
      "epoch 30 | loss: 0.33595 | val_0_f1: 0.82474 | val_0_gini: 0.85894 |  0:00:08s\n",
      "epoch 31 | loss: 0.30727 | val_0_f1: 0.78399 | val_0_gini: 0.84379 |  0:00:08s\n",
      "epoch 32 | loss: 0.30623 | val_0_f1: 0.77944 | val_0_gini: 0.83033 |  0:00:08s\n",
      "epoch 33 | loss: 0.30576 | val_0_f1: 0.78205 | val_0_gini: 0.82323 |  0:00:09s\n",
      "epoch 34 | loss: 0.30864 | val_0_f1: 0.78098 | val_0_gini: 0.82115 |  0:00:09s\n",
      "epoch 35 | loss: 0.3088  | val_0_f1: 0.80161 | val_0_gini: 0.82598 |  0:00:09s\n",
      "epoch 36 | loss: 0.3308  | val_0_f1: 0.83815 | val_0_gini: 0.85951 |  0:00:09s\n",
      "epoch 37 | loss: 0.30672 | val_0_f1: 0.84944 | val_0_gini: 0.86598 |  0:00:10s\n",
      "epoch 38 | loss: 0.3052  | val_0_f1: 0.86465 | val_0_gini: 0.8782  |  0:00:10s\n",
      "epoch 39 | loss: 0.30782 | val_0_f1: 0.87124 | val_0_gini: 0.87395 |  0:00:10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 | loss: 0.28235 | val_0_f1: 0.85352 | val_0_gini: 0.87656 |  0:00:10s\n",
      "epoch 41 | loss: 0.25822 | val_0_f1: 0.85278 | val_0_gini: 0.89008 |  0:00:11s\n",
      "epoch 42 | loss: 0.24811 | val_0_f1: 0.82363 | val_0_gini: 0.8884  |  0:00:11s\n",
      "epoch 43 | loss: 0.26516 | val_0_f1: 0.83066 | val_0_gini: 0.90018 |  0:00:11s\n",
      "epoch 44 | loss: 0.27142 | val_0_f1: 0.84284 | val_0_gini: 0.89937 |  0:00:11s\n",
      "epoch 45 | loss: 0.27783 | val_0_f1: 0.82824 | val_0_gini: 0.88459 |  0:00:12s\n",
      "epoch 46 | loss: 0.24563 | val_0_f1: 0.80611 | val_0_gini: 0.86412 |  0:00:12s\n",
      "epoch 47 | loss: 0.26662 | val_0_f1: 0.82712 | val_0_gini: 0.85996 |  0:00:12s\n",
      "epoch 48 | loss: 0.26367 | val_0_f1: 0.82274 | val_0_gini: 0.83856 |  0:00:12s\n",
      "epoch 49 | loss: 0.28358 | val_0_f1: 0.81676 | val_0_gini: 0.81582 |  0:00:13s\n",
      "epoch 50 | loss: 0.28519 | val_0_f1: 0.81911 | val_0_gini: 0.83348 |  0:00:13s\n",
      "epoch 51 | loss: 0.26953 | val_0_f1: 0.80763 | val_0_gini: 0.7884  |  0:00:13s\n",
      "epoch 52 | loss: 0.24897 | val_0_f1: 0.81594 | val_0_gini: 0.7537  |  0:00:13s\n",
      "epoch 53 | loss: 0.24692 | val_0_f1: 0.82062 | val_0_gini: 0.77916 |  0:00:14s\n",
      "\n",
      "Early stopping occured at epoch 53 with best_epoch = 43 and best_val_0_gini = 0.90018\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.30264 | val_0_f1: 0.45255 | val_0_gini: 0.14427 |  0:00:00s\n",
      "epoch 1  | loss: 1.04057 | val_0_f1: 0.34928 | val_0_gini: 0.10127 |  0:00:00s\n",
      "epoch 2  | loss: 0.98541 | val_0_f1: 0.45113 | val_0_gini: 0.28022 |  0:00:00s\n",
      "epoch 3  | loss: 0.85432 | val_0_f1: 0.50141 | val_0_gini: 0.20636 |  0:00:00s\n",
      "epoch 4  | loss: 0.79687 | val_0_f1: 0.53547 | val_0_gini: 0.24235 |  0:00:01s\n",
      "epoch 5  | loss: 0.72245 | val_0_f1: 0.59255 | val_0_gini: 0.26577 |  0:00:01s\n",
      "epoch 6  | loss: 0.68297 | val_0_f1: 0.64603 | val_0_gini: 0.29948 |  0:00:01s\n",
      "epoch 7  | loss: 0.64204 | val_0_f1: 0.65334 | val_0_gini: 0.46834 |  0:00:01s\n",
      "epoch 8  | loss: 0.60271 | val_0_f1: 0.6759  | val_0_gini: 0.51024 |  0:00:02s\n",
      "epoch 9  | loss: 0.57291 | val_0_f1: 0.6691  | val_0_gini: 0.61016 |  0:00:02s\n",
      "epoch 10 | loss: 0.53642 | val_0_f1: 0.69618 | val_0_gini: 0.6823  |  0:00:02s\n",
      "epoch 11 | loss: 0.48281 | val_0_f1: 0.69374 | val_0_gini: 0.65278 |  0:00:02s\n",
      "epoch 12 | loss: 0.44391 | val_0_f1: 0.70641 | val_0_gini: 0.63005 |  0:00:03s\n",
      "epoch 13 | loss: 0.465   | val_0_f1: 0.73113 | val_0_gini: 0.67806 |  0:00:03s\n",
      "epoch 14 | loss: 0.45992 | val_0_f1: 0.75358 | val_0_gini: 0.62607 |  0:00:03s\n",
      "epoch 15 | loss: 0.44238 | val_0_f1: 0.74226 | val_0_gini: 0.62615 |  0:00:03s\n",
      "epoch 16 | loss: 0.42713 | val_0_f1: 0.7239  | val_0_gini: 0.62857 |  0:00:04s\n",
      "epoch 17 | loss: 0.39364 | val_0_f1: 0.70322 | val_0_gini: 0.65106 |  0:00:04s\n",
      "epoch 18 | loss: 0.38005 | val_0_f1: 0.69865 | val_0_gini: 0.59983 |  0:00:04s\n",
      "epoch 19 | loss: 0.38385 | val_0_f1: 0.71563 | val_0_gini: 0.54873 |  0:00:04s\n",
      "epoch 20 | loss: 0.37907 | val_0_f1: 0.73072 | val_0_gini: 0.57085 |  0:00:05s\n",
      "\n",
      "Early stopping occured at epoch 20 with best_epoch = 10 and best_val_0_gini = 0.6823\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.30089 | val_0_f1: 0.42677 | val_0_gini: 0.1377  |  0:00:00s\n",
      "epoch 1  | loss: 1.02059 | val_0_f1: 0.43766 | val_0_gini: 0.29598 |  0:00:00s\n",
      "epoch 2  | loss: 0.89835 | val_0_f1: 0.44472 | val_0_gini: 0.14951 |  0:00:00s\n",
      "epoch 3  | loss: 0.83519 | val_0_f1: 0.55243 | val_0_gini: 0.21232 |  0:00:00s\n",
      "epoch 4  | loss: 0.74361 | val_0_f1: 0.57771 | val_0_gini: 0.28364 |  0:00:01s\n",
      "epoch 5  | loss: 0.67401 | val_0_f1: 0.58704 | val_0_gini: 0.26839 |  0:00:01s\n",
      "epoch 6  | loss: 0.61793 | val_0_f1: 0.57102 | val_0_gini: 0.27819 |  0:00:01s\n",
      "epoch 7  | loss: 0.57295 | val_0_f1: 0.56101 | val_0_gini: 0.2991  |  0:00:01s\n",
      "epoch 8  | loss: 0.55464 | val_0_f1: 0.57341 | val_0_gini: 0.35005 |  0:00:02s\n",
      "epoch 9  | loss: 0.50759 | val_0_f1: 0.59477 | val_0_gini: 0.36437 |  0:00:02s\n",
      "epoch 10 | loss: 0.52508 | val_0_f1: 0.59328 | val_0_gini: 0.345   |  0:00:02s\n",
      "epoch 11 | loss: 0.43594 | val_0_f1: 0.6103  | val_0_gini: 0.30093 |  0:00:02s\n",
      "epoch 12 | loss: 0.46572 | val_0_f1: 0.61807 | val_0_gini: 0.3916  |  0:00:03s\n",
      "epoch 13 | loss: 0.44082 | val_0_f1: 0.61343 | val_0_gini: 0.48998 |  0:00:03s\n",
      "epoch 14 | loss: 0.41803 | val_0_f1: 0.68706 | val_0_gini: 0.5642  |  0:00:03s\n",
      "epoch 15 | loss: 0.42705 | val_0_f1: 0.6815  | val_0_gini: 0.57414 |  0:00:03s\n",
      "epoch 16 | loss: 0.38862 | val_0_f1: 0.70257 | val_0_gini: 0.5618  |  0:00:04s\n",
      "epoch 17 | loss: 0.43559 | val_0_f1: 0.72499 | val_0_gini: 0.59706 |  0:00:04s\n",
      "epoch 18 | loss: 0.39107 | val_0_f1: 0.7533  | val_0_gini: 0.62264 |  0:00:04s\n",
      "epoch 19 | loss: 0.41333 | val_0_f1: 0.75999 | val_0_gini: 0.62298 |  0:00:04s\n",
      "epoch 20 | loss: 0.386   | val_0_f1: 0.75193 | val_0_gini: 0.62886 |  0:00:05s\n",
      "epoch 21 | loss: 0.38784 | val_0_f1: 0.72369 | val_0_gini: 0.62405 |  0:00:05s\n",
      "epoch 22 | loss: 0.39224 | val_0_f1: 0.72257 | val_0_gini: 0.64417 |  0:00:05s\n",
      "epoch 23 | loss: 0.3796  | val_0_f1: 0.74814 | val_0_gini: 0.63105 |  0:00:05s\n",
      "epoch 24 | loss: 0.35389 | val_0_f1: 0.75486 | val_0_gini: 0.68158 |  0:00:05s\n",
      "epoch 25 | loss: 0.3529  | val_0_f1: 0.78149 | val_0_gini: 0.71799 |  0:00:06s\n",
      "epoch 26 | loss: 0.32569 | val_0_f1: 0.75792 | val_0_gini: 0.71981 |  0:00:06s\n",
      "epoch 27 | loss: 0.30219 | val_0_f1: 0.74353 | val_0_gini: 0.70423 |  0:00:06s\n",
      "epoch 28 | loss: 0.33481 | val_0_f1: 0.76465 | val_0_gini: 0.70866 |  0:00:06s\n",
      "epoch 29 | loss: 0.33241 | val_0_f1: 0.78705 | val_0_gini: 0.72053 |  0:00:07s\n",
      "epoch 30 | loss: 0.30514 | val_0_f1: 0.80604 | val_0_gini: 0.71077 |  0:00:07s\n",
      "epoch 31 | loss: 0.34592 | val_0_f1: 0.80937 | val_0_gini: 0.70692 |  0:00:07s\n",
      "epoch 32 | loss: 0.29547 | val_0_f1: 0.81819 | val_0_gini: 0.70029 |  0:00:07s\n",
      "epoch 33 | loss: 0.2874  | val_0_f1: 0.82274 | val_0_gini: 0.70945 |  0:00:08s\n",
      "epoch 34 | loss: 0.30691 | val_0_f1: 0.80606 | val_0_gini: 0.68444 |  0:00:08s\n",
      "epoch 35 | loss: 0.29922 | val_0_f1: 0.81509 | val_0_gini: 0.67611 |  0:00:08s\n",
      "epoch 36 | loss: 0.25008 | val_0_f1: 0.80832 | val_0_gini: 0.66929 |  0:00:08s\n",
      "epoch 37 | loss: 0.29782 | val_0_f1: 0.80546 | val_0_gini: 0.68919 |  0:00:09s\n",
      "epoch 38 | loss: 0.26903 | val_0_f1: 0.79253 | val_0_gini: 0.6865  |  0:00:09s\n",
      "epoch 39 | loss: 0.27908 | val_0_f1: 0.80582 | val_0_gini: 0.69474 |  0:00:09s\n",
      "\n",
      "Early stopping occured at epoch 39 with best_epoch = 29 and best_val_0_gini = 0.72053\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.3035  | val_0_f1: 0.45838 | val_0_gini: 0.12216 |  0:00:00s\n",
      "epoch 1  | loss: 1.01441 | val_0_f1: 0.33801 | val_0_gini: 0.20199 |  0:00:00s\n",
      "epoch 2  | loss: 0.94586 | val_0_f1: 0.35777 | val_0_gini: 0.22237 |  0:00:00s\n",
      "epoch 3  | loss: 0.87985 | val_0_f1: 0.43137 | val_0_gini: 0.30129 |  0:00:00s\n",
      "epoch 4  | loss: 0.77157 | val_0_f1: 0.52267 | val_0_gini: 0.37586 |  0:00:01s\n",
      "epoch 5  | loss: 0.71161 | val_0_f1: 0.58652 | val_0_gini: 0.41202 |  0:00:01s\n",
      "epoch 6  | loss: 0.65898 | val_0_f1: 0.57881 | val_0_gini: 0.40411 |  0:00:01s\n",
      "epoch 7  | loss: 0.62181 | val_0_f1: 0.54172 | val_0_gini: 0.39374 |  0:00:01s\n",
      "epoch 8  | loss: 0.57909 | val_0_f1: 0.42572 | val_0_gini: 0.32456 |  0:00:02s\n",
      "epoch 9  | loss: 0.52327 | val_0_f1: 0.4273  | val_0_gini: 0.35099 |  0:00:02s\n",
      "epoch 10 | loss: 0.51822 | val_0_f1: 0.49225 | val_0_gini: 0.40802 |  0:00:02s\n",
      "epoch 11 | loss: 0.50763 | val_0_f1: 0.57178 | val_0_gini: 0.4522  |  0:00:02s\n",
      "epoch 12 | loss: 0.4989  | val_0_f1: 0.66788 | val_0_gini: 0.47166 |  0:00:03s\n",
      "epoch 13 | loss: 0.49202 | val_0_f1: 0.69351 | val_0_gini: 0.47568 |  0:00:03s\n",
      "epoch 14 | loss: 0.42981 | val_0_f1: 0.67399 | val_0_gini: 0.47635 |  0:00:03s\n",
      "epoch 15 | loss: 0.49586 | val_0_f1: 0.68664 | val_0_gini: 0.56406 |  0:00:03s\n",
      "epoch 16 | loss: 0.44373 | val_0_f1: 0.66197 | val_0_gini: 0.58244 |  0:00:04s\n",
      "epoch 17 | loss: 0.43487 | val_0_f1: 0.66819 | val_0_gini: 0.59084 |  0:00:04s\n",
      "epoch 18 | loss: 0.40251 | val_0_f1: 0.66592 | val_0_gini: 0.57967 |  0:00:04s\n",
      "epoch 19 | loss: 0.39359 | val_0_f1: 0.66658 | val_0_gini: 0.58487 |  0:00:04s\n",
      "epoch 20 | loss: 0.41401 | val_0_f1: 0.68076 | val_0_gini: 0.59823 |  0:00:05s\n",
      "epoch 21 | loss: 0.41432 | val_0_f1: 0.71421 | val_0_gini: 0.60649 |  0:00:05s\n",
      "epoch 22 | loss: 0.35923 | val_0_f1: 0.72963 | val_0_gini: 0.63164 |  0:00:05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | loss: 0.384   | val_0_f1: 0.75332 | val_0_gini: 0.65359 |  0:00:05s\n",
      "epoch 24 | loss: 0.35723 | val_0_f1: 0.73343 | val_0_gini: 0.62995 |  0:00:06s\n",
      "epoch 25 | loss: 0.38265 | val_0_f1: 0.72484 | val_0_gini: 0.64066 |  0:00:06s\n",
      "epoch 26 | loss: 0.38729 | val_0_f1: 0.7566  | val_0_gini: 0.64762 |  0:00:06s\n",
      "epoch 27 | loss: 0.33492 | val_0_f1: 0.759   | val_0_gini: 0.67635 |  0:00:06s\n",
      "epoch 28 | loss: 0.35747 | val_0_f1: 0.77481 | val_0_gini: 0.69284 |  0:00:07s\n",
      "epoch 29 | loss: 0.30605 | val_0_f1: 0.76349 | val_0_gini: 0.69742 |  0:00:07s\n",
      "epoch 30 | loss: 0.27803 | val_0_f1: 0.78045 | val_0_gini: 0.70522 |  0:00:07s\n",
      "epoch 31 | loss: 0.32501 | val_0_f1: 0.77079 | val_0_gini: 0.70594 |  0:00:07s\n",
      "epoch 32 | loss: 0.3201  | val_0_f1: 0.75746 | val_0_gini: 0.69886 |  0:00:08s\n",
      "epoch 33 | loss: 0.25266 | val_0_f1: 0.77785 | val_0_gini: 0.70142 |  0:00:08s\n",
      "epoch 34 | loss: 0.27639 | val_0_f1: 0.77304 | val_0_gini: 0.72664 |  0:00:08s\n",
      "epoch 35 | loss: 0.26296 | val_0_f1: 0.77949 | val_0_gini: 0.67573 |  0:00:08s\n",
      "epoch 36 | loss: 0.2381  | val_0_f1: 0.79283 | val_0_gini: 0.685   |  0:00:08s\n",
      "epoch 37 | loss: 0.29506 | val_0_f1: 0.80753 | val_0_gini: 0.7087  |  0:00:09s\n",
      "epoch 38 | loss: 0.27141 | val_0_f1: 0.79875 | val_0_gini: 0.72167 |  0:00:09s\n",
      "epoch 39 | loss: 0.23025 | val_0_f1: 0.8108  | val_0_gini: 0.71464 |  0:00:09s\n",
      "epoch 40 | loss: 0.31047 | val_0_f1: 0.82982 | val_0_gini: 0.74157 |  0:00:09s\n",
      "epoch 41 | loss: 0.24898 | val_0_f1: 0.82335 | val_0_gini: 0.81069 |  0:00:10s\n",
      "epoch 42 | loss: 0.26033 | val_0_f1: 0.82704 | val_0_gini: 0.80358 |  0:00:10s\n",
      "epoch 43 | loss: 0.26441 | val_0_f1: 0.82747 | val_0_gini: 0.79425 |  0:00:10s\n",
      "epoch 44 | loss: 0.26986 | val_0_f1: 0.83327 | val_0_gini: 0.80926 |  0:00:10s\n",
      "epoch 45 | loss: 0.26981 | val_0_f1: 0.82352 | val_0_gini: 0.81157 |  0:00:11s\n",
      "epoch 46 | loss: 0.23655 | val_0_f1: 0.82764 | val_0_gini: 0.83954 |  0:00:11s\n",
      "epoch 47 | loss: 0.21623 | val_0_f1: 0.83204 | val_0_gini: 0.84271 |  0:00:11s\n",
      "epoch 48 | loss: 0.24027 | val_0_f1: 0.81161 | val_0_gini: 0.83886 |  0:00:11s\n",
      "epoch 49 | loss: 0.28213 | val_0_f1: 0.80404 | val_0_gini: 0.83747 |  0:00:12s\n",
      "epoch 50 | loss: 0.27395 | val_0_f1: 0.79783 | val_0_gini: 0.83595 |  0:00:12s\n",
      "epoch 51 | loss: 0.27554 | val_0_f1: 0.81567 | val_0_gini: 0.8418  |  0:00:12s\n",
      "epoch 52 | loss: 0.2752  | val_0_f1: 0.81188 | val_0_gini: 0.84823 |  0:00:12s\n",
      "epoch 53 | loss: 0.23885 | val_0_f1: 0.8205  | val_0_gini: 0.83022 |  0:00:12s\n",
      "epoch 54 | loss: 0.23959 | val_0_f1: 0.79311 | val_0_gini: 0.8093  |  0:00:13s\n",
      "epoch 55 | loss: 0.26416 | val_0_f1: 0.7947  | val_0_gini: 0.8055  |  0:00:13s\n",
      "epoch 56 | loss: 0.25145 | val_0_f1: 0.80134 | val_0_gini: 0.82062 |  0:00:13s\n",
      "epoch 57 | loss: 0.23541 | val_0_f1: 0.79564 | val_0_gini: 0.81554 |  0:00:13s\n",
      "epoch 58 | loss: 0.23911 | val_0_f1: 0.79847 | val_0_gini: 0.81646 |  0:00:14s\n",
      "epoch 59 | loss: 0.22725 | val_0_f1: 0.81372 | val_0_gini: 0.80957 |  0:00:14s\n",
      "epoch 60 | loss: 0.22621 | val_0_f1: 0.8105  | val_0_gini: 0.80624 |  0:00:14s\n",
      "epoch 61 | loss: 0.26265 | val_0_f1: 0.80805 | val_0_gini: 0.82093 |  0:00:15s\n",
      "epoch 62 | loss: 0.21443 | val_0_f1: 0.84415 | val_0_gini: 0.84348 |  0:00:15s\n",
      "\n",
      "Early stopping occured at epoch 62 with best_epoch = 52 and best_val_0_gini = 0.84823\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.31199 | val_0_f1: 0.45467 | val_0_gini: 0.16068 |  0:00:00s\n",
      "epoch 1  | loss: 1.00363 | val_0_f1: 0.39936 | val_0_gini: 0.13275 |  0:00:00s\n",
      "epoch 2  | loss: 0.89562 | val_0_f1: 0.40305 | val_0_gini: 0.2012  |  0:00:00s\n",
      "epoch 3  | loss: 0.80265 | val_0_f1: 0.43893 | val_0_gini: 0.21228 |  0:00:01s\n",
      "epoch 4  | loss: 0.73366 | val_0_f1: 0.42631 | val_0_gini: 0.23999 |  0:00:01s\n",
      "epoch 5  | loss: 0.67709 | val_0_f1: 0.3723  | val_0_gini: 0.06989 |  0:00:01s\n",
      "epoch 6  | loss: 0.63651 | val_0_f1: 0.38838 | val_0_gini: 0.13351 |  0:00:01s\n",
      "epoch 7  | loss: 0.57471 | val_0_f1: 0.36652 | val_0_gini: 0.21773 |  0:00:02s\n",
      "epoch 8  | loss: 0.55666 | val_0_f1: 0.37776 | val_0_gini: 0.31112 |  0:00:02s\n",
      "epoch 9  | loss: 0.50437 | val_0_f1: 0.44235 | val_0_gini: 0.36645 |  0:00:02s\n",
      "epoch 10 | loss: 0.51264 | val_0_f1: 0.4709  | val_0_gini: 0.53436 |  0:00:02s\n",
      "epoch 11 | loss: 0.50793 | val_0_f1: 0.55816 | val_0_gini: 0.62069 |  0:00:03s\n",
      "epoch 12 | loss: 0.45833 | val_0_f1: 0.59804 | val_0_gini: 0.66628 |  0:00:03s\n",
      "epoch 13 | loss: 0.39369 | val_0_f1: 0.61781 | val_0_gini: 0.6734  |  0:00:03s\n",
      "epoch 14 | loss: 0.3702  | val_0_f1: 0.62823 | val_0_gini: 0.69933 |  0:00:03s\n",
      "epoch 15 | loss: 0.42851 | val_0_f1: 0.66681 | val_0_gini: 0.66783 |  0:00:04s\n",
      "epoch 16 | loss: 0.38519 | val_0_f1: 0.67839 | val_0_gini: 0.66155 |  0:00:04s\n",
      "epoch 17 | loss: 0.38557 | val_0_f1: 0.68195 | val_0_gini: 0.67584 |  0:00:04s\n",
      "epoch 18 | loss: 0.3716  | val_0_f1: 0.70034 | val_0_gini: 0.62836 |  0:00:04s\n",
      "epoch 19 | loss: 0.39303 | val_0_f1: 0.71554 | val_0_gini: 0.63186 |  0:00:05s\n",
      "epoch 20 | loss: 0.3595  | val_0_f1: 0.70483 | val_0_gini: 0.67136 |  0:00:05s\n",
      "epoch 21 | loss: 0.3271  | val_0_f1: 0.71398 | val_0_gini: 0.72953 |  0:00:05s\n",
      "epoch 22 | loss: 0.33289 | val_0_f1: 0.7544  | val_0_gini: 0.77207 |  0:00:06s\n",
      "epoch 23 | loss: 0.32516 | val_0_f1: 0.77941 | val_0_gini: 0.80911 |  0:00:06s\n",
      "epoch 24 | loss: 0.30507 | val_0_f1: 0.77394 | val_0_gini: 0.77315 |  0:00:06s\n",
      "epoch 25 | loss: 0.32812 | val_0_f1: 0.7589  | val_0_gini: 0.76559 |  0:00:06s\n",
      "epoch 26 | loss: 0.31128 | val_0_f1: 0.76512 | val_0_gini: 0.77529 |  0:00:07s\n",
      "epoch 27 | loss: 0.28093 | val_0_f1: 0.75791 | val_0_gini: 0.77408 |  0:00:07s\n",
      "epoch 28 | loss: 0.34068 | val_0_f1: 0.75182 | val_0_gini: 0.78975 |  0:00:07s\n",
      "epoch 29 | loss: 0.33021 | val_0_f1: 0.74798 | val_0_gini: 0.78756 |  0:00:07s\n",
      "epoch 30 | loss: 0.28885 | val_0_f1: 0.74626 | val_0_gini: 0.77199 |  0:00:08s\n",
      "epoch 31 | loss: 0.27985 | val_0_f1: 0.76148 | val_0_gini: 0.78368 |  0:00:08s\n",
      "epoch 32 | loss: 0.29908 | val_0_f1: 0.77178 | val_0_gini: 0.79231 |  0:00:08s\n",
      "epoch 33 | loss: 0.30084 | val_0_f1: 0.77433 | val_0_gini: 0.78244 |  0:00:08s\n",
      "\n",
      "Early stopping occured at epoch 33 with best_epoch = 23 and best_val_0_gini = 0.80911\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=7, random_state=13, shuffle=True)\n",
    "\n",
    "preds = []\n",
    "cv_preds = []\n",
    "for train_index, test_index in skf.split(X_t, y):\n",
    "    xtrain = X_t[train_index]\n",
    "    ytrain = y[train_index]\n",
    "    xval = X_t[test_index]\n",
    "    yval = y[test_index]\n",
    "    clf = TabNetClassifier(seed=13) \n",
    "    clf.fit(\n",
    "        xtrain, ytrain, eval_set=[(xval, yval)], weights=1,\n",
    "        eval_metric=[F1, Gini]\n",
    "    )\n",
    "    cv_preds.append(clf.predict_proba(X_t))\n",
    "    preds.append(clf.predict_proba(X_test_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8423529411764706"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install --upgrade pytorch_tabnet\n",
    "np.mean(np.stack(cv_preds).mean(0).argmax(1)+1==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['NSP'] = np.stack(preds).mean(0).argmax(1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"prediction_tabnet_1.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
